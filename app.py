import streamlit as st
import pandas as pd
import openai
from sentence_transformers import util
import torch
import io
import re
import time
import math
from sklearn.cluster import AgglomerativeClustering
import numpy as np

# Ustawienia strony Streamlit
st.set_page_config(page_title="Planer Tre≈õci SEO", layout="wide")

# --- Funkcje pomocnicze ---

@st.cache_data(show_spinner=False)
def get_openai_embeddings(texts_tuple, api_key, batch_size=256):
    """
    Generuje wektory (embeddings) za pomocƒÖ API OpenAI, u≈ºywajƒÖc batchingu
    dla obs≈Çugi du≈ºych list i unikniƒôcia b≈Çƒôd√≥w API.
    """
    texts = list(texts_tuple)
    client = openai.OpenAI(api_key=api_key)
    all_embeddings = []
    
    clean_texts = [str(text).strip() if pd.notna(text) and str(text).strip() else " " for text in texts]

    num_batches = math.ceil(len(clean_texts) / batch_size)
    
    for i in range(num_batches):
        start_index = i * batch_size
        end_index = start_index + batch_size
        batch = clean_texts[start_index:end_index]
        
        try:
            response = client.embeddings.create(input=batch, model="text-embedding-3-large")
            all_embeddings.extend([item.embedding for item in response.data])
            time.sleep(1)

        except Exception as e:
            st.error(f"B≈ÇƒÖd podczas przetwarzania batcha nr {i+1}: {e}")
            return None
            
    return all_embeddings

def cluster_keywords(keywords_df, embeddings, cluster_threshold=0.85):
    """
    Klasteryzuje frazy kluczowe na podstawie podobie≈Ñstwa semantycznego.
    Zwraca DataFrame z przypisanymi klastrami i HEAD keywords.
    """
    if len(keywords_df) == 0:
        return keywords_df
    
    # Upewniamy siƒô, ≈ºe kolumna Wolumen istnieje
    if 'Volume' not in keywords_df.columns and 'Wolumen' not in keywords_df.columns:
        keywords_df['Wolumen'] = 0
    elif 'Volume' in keywords_df.columns and 'Wolumen' not in keywords_df.columns:
        keywords_df['Wolumen'] = keywords_df['Volume']
    
    # Konwersja do numpy array
    embeddings_array = np.array(embeddings)
    
    # Obliczanie macierzy podobie≈Ñstwa
    similarity_matrix = util.cos_sim(embeddings_array, embeddings_array).numpy()
    
    # Konwersja podobie≈Ñstwa na odleg≈Ço≈õƒá
    distance_matrix = 1 - similarity_matrix
    
    # Klasteryzacja hierarchiczna
    clustering = AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=1 - cluster_threshold,
        metric='precomputed',
        linkage='average'
    )
    
    cluster_labels = clustering.fit_predict(distance_matrix)
    
    # Dodanie informacji o klastrach do DataFrame
    keywords_df['Klaster_ID'] = cluster_labels
    
    # Wyznaczenie HEAD keyword dla ka≈ºdego klastra (najwy≈ºszy wolumen)
    def get_head_keyword(group):
        if 'Wolumen' in group.columns and len(group) > 0:
            head_idx = group['Wolumen'].idxmax()
            keyword_col = 'Keyword' if 'Keyword' in group.columns else 'S≈Çowo kluczowe'
            group['HEAD_Keyword'] = group.loc[head_idx, keyword_col]
            group['Typ_w_klastrze'] = 'RELATED'
            group.loc[head_idx, 'Typ_w_klastrze'] = 'HEAD'
            group['Liczba_fraz_w_klastrze'] = len(group)
        return group
    
    keywords_df = keywords_df.groupby('Klaster_ID', group_keys=False).apply(get_head_keyword)
    
    return keywords_df

def detect_search_intent(keyword):
    """
    Wykrywa intencjƒô wyszukiwania na podstawie wzorc√≥w w frazie kluczowej.
    """
    keyword_lower = keyword.lower()
    
    # Wzorce informacyjne
    info_patterns = ['jak', 'co to', 'dlaczego', 'kiedy', 'gdzie', 'czy', 'jakie', 'r√≥≈ºnica']
    # Wzorce transakcyjne
    trans_patterns = ['kup', 'cena', 'sklep', 'oferta', 'promocja', 'tani', 'najleps']
    # Wzorce nawigacyjne
    nav_patterns = ['logowanie', 'kontakt', 'strona', 'oficjalna']
    
    if any(pattern in keyword_lower for pattern in trans_patterns):
        return 'Transakcyjna'
    elif any(pattern in keyword_lower for pattern in nav_patterns):
        return 'Nawigacyjna'
    elif any(pattern in keyword_lower for pattern in info_patterns):
        return 'Informacyjna'
    else:
        return 'Mieszana'

def calculate_priority_score(row):
    """
    Oblicza scoring priorytetu dla danej frazy kluczowej.
    Wz√≥r: (Wolumen √ó 0.5) + (Pozycja_odwr√≥cona √ó 0.3) + (Podobie≈Ñstwo √ó 0.2)
    """
    volume_score = min(row.get('Wolumen', 0) / 1000, 100)  # Normalizacja do 100
    
    # Scoring pozycji (im gorsza pozycja, tym wiƒôkszy potencja≈Ç)
    position = row.get('Aktualna_pozycja', 0)
    if position == 0:  # Nie rankuje
        position_score = 100
    elif position > 20:
        position_score = 80
    elif position > 10:
        position_score = 60
    elif position > 5:
        position_score = 40
    else:
        position_score = 20
    
    # Scoring podobie≈Ñstwa (ni≈ºsze = nowy temat = wy≈ºszy priorytet)
    similarity_score = (1 - row.get('Podobie≈Ñstwo', 0)) * 100
    
    priority = (volume_score * 0.5) + (position_score * 0.3) + (similarity_score * 0.2)
    return round(priority, 2)

def find_first_competitor_url(row):
    """Znajduje pierwszy dostƒôpny URL konkurenta w danym wierszu."""
    for col in row.index:
        if isinstance(col, str) and col.endswith(': URL') and pd.notna(row[col]):
            return row[col]
    return "Brak"

def generate_titles(api_key, keyword, volume, competitor_url, related_keywords="", max_retries=3):
    """Generuje tytu≈Çy za pomocƒÖ API OpenAI (GPT-4o)."""
    client = openai.OpenAI(api_key=api_key)
    
    related_info = f"\n- PowiƒÖzane frazy do uwzglƒôdnienia: {related_keywords}" if related_keywords else ""
    
    prompt = f"""
Jeste≈õ ekspertem SEO i copywriterem specjalizujƒÖcym siƒô w tworzeniu anga≈ºujƒÖcych tytu≈Ç√≥w na polskojƒôzyczne blogi.
Przeanalizuj poni≈ºsze dane:
- G≈Ç√≥wne s≈Çowo kluczowe: "{keyword}"
- Miesiƒôczny wolumen wyszukiwania: {volume}{related_info}
- Artyku≈Ç konkurencji: {competitor_url}

Twoje zadanie: Zaproponuj 3 unikalne tytu≈Çy artyku≈Ç√≥w blogowych.
Zasady:
1. G≈Ç√≥wny tytu≈Ç musi zawieraƒá dok≈ÇadnƒÖ frazƒô kluczowƒÖ: "{keyword}".
2. Je≈õli sƒÖ powiƒÖzane frazy, w≈ÇƒÖcz je naturalnie w tre≈õƒá tytu≈Ç√≥w.
3. Tytu≈Çy muszƒÖ mieƒá charakter informacyjny lub poradnikowy (np. "Jak...", "Co to jest...").
4. Stosuj polskie zasady pisowni ‚Äì tylko pierwsza litera w tytule wielka.
5. Zamiast dwukropka u≈ºywaj my≈õlnika.
6. Zwr√≥ƒá odpowied≈∫ wy≈ÇƒÖcznie w formie listy numerowanej.
"""
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Jeste≈õ ekspertem SEO i copywriterem."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=400
            )
            content = response.choices[0].message.content
            titles = re.findall(r'\d+\.\s*(.*)', content)
            
            while len(titles) < 3:
                titles.append("---")
            
            time.sleep(0.5)
            return titles[:3]
            
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                time.sleep(wait_time)
                continue
            else:
                st.warning(f"B≈ÇƒÖd podczas generowania tytu≈Ç√≥w dla '{keyword}': {e}")
                return ["B≈ÇƒÖd API", "B≈ÇƒÖd API", "B≈ÇƒÖd API"]

def validate_api_key(api_key):
    """Waliduje klucz API OpenAI."""
    try:
        client = openai.OpenAI(api_key=api_key)
        client.models.list()
        return True
    except Exception as e:
        st.error(f"Nieprawid≈Çowy klucz API OpenAI: {e}")
        return False

# --- Interfejs U≈ºytkownika (UI) ---
st.title("üöÄ Planer Tre≈õci SEO [Wersja Hybrydowa v8 - ULTIMATE]")
st.markdown("‚ú® Wersja z klasteryzacjƒÖ, analizƒÖ intencji, scoring'iem priorytet√≥w i rozszerzonym statusem rankingu.")

col1, col2 = st.columns(2)
with col1:
    st.header("1. Konfiguracja")
    num_to_generate = st.number_input("Liczba nowych artyku≈Ç√≥w do wygenerowania", min_value=1, value=20)
    similarity_threshold = st.slider("Pr√≥g podobie≈Ñstwa dla optymalizacji", min_value=0.7, max_value=1.0, value=0.8, step=0.01)
    cluster_threshold = st.slider("Pr√≥g podobie≈Ñstwa dla klasteryzacji fraz", min_value=0.75, max_value=0.95, value=0.85, step=0.01, 
                                  help="Frazy powy≈ºej tego progu zostanƒÖ zgrupowane w jeden klaster")
    enable_clustering = st.checkbox("W≈ÇƒÖcz klasteryzacjƒô fraz kluczowych", value=True)
    
with col2:
    st.header("2. Wgraj pliki CSV")
    content_gap_file = st.file_uploader("1. Wgraj plik CSV z analizƒÖ Content Gap", type="csv")
    my_articles_file = st.file_uploader("2. Wgraj plik CSV z listƒÖ swoich artyku≈Ç√≥w", type="csv")
    ranking_file = st.file_uploader("3. Wgraj plik CSV z aktualnym rankingiem", type="csv")

# --- Logika Aplikacji ---
if st.button("Uruchom Analizƒô HybrydowƒÖ", type="primary"):
    openai_api_key = st.secrets.get("OPENAI_API_KEY")
    
    if not openai_api_key:
        st.error("Klucz API OpenAI nie zosta≈Ç znaleziony w sekretach Streamlit!")
        st.stop()
    
    if not validate_api_key(openai_api_key):
        st.stop()
    
    if not all([content_gap_file, my_articles_file, ranking_file]):
        st.warning("Upewnij siƒô, ≈ºe wgra≈Çe≈õ wszystkie trzy pliki CSV.")
        st.stop()
    
    with st.spinner("Przeprowadzam analizƒô..."):
        try:
            df_gap = pd.read_csv(content_gap_file).dropna(subset=['Keyword']).astype({'Keyword': str})
            df_articles = pd.read_csv(my_articles_file)
            df_ranking = pd.read_csv(ranking_file)
            
            df_articles.rename(columns={'Address': 'URL', 'Title 1': 'Title'}, inplace=True)
            df_articles.dropna(subset=['Title', 'URL'], inplace=True)
            df_articles = df_articles[df_articles['Title'].str.strip() != ''].reset_index(drop=True)
            df_articles = df_articles[~df_articles['Title'].str.contains("Bot Verification|Strona|Kategoria", na=False, case=False)].reset_index(drop=True)
            df_ranking.dropna(subset=['S≈Çowo kluczowe', 'Adres URL'], inplace=True)
            
            # Sprawdzanie czy ranking ma kolumnƒô pozycji
            position_col = None
            for col in df_ranking.columns:
                if 'pozycj' in col.lower() or 'position' in col.lower():
                    position_col = col
                    break
            
            st.info(f"Wczytano {len(df_gap)} s≈Ç√≥w kluczowych, {len(df_articles)} artyku≈Ç√≥w i {len(df_ranking)} rankingowych s≈Ç√≥w kluczowych.")
        except Exception as e:
            st.error(f"B≈ÇƒÖd podczas wczytywania plik√≥w CSV: {e}")
            st.stop()

        # Tworzenie mapy rankingu z pozycjami
        ranking_map = {}
        for _, row in df_ranking.iterrows():
            keyword = str(row['S≈Çowo kluczowe']).lower()
            url = row['Adres URL']
            position = row[position_col] if position_col and pd.notna(row.get(position_col)) else 0
            ranking_map[keyword] = {'url': url, 'position': position}
        
        results = []
        keywords_for_semantic_check = []
        
        # Pierwsza faza: mapowanie na podstawie rankingu
        for _, row in df_gap.iterrows():
            keyword_lower = str(row['Keyword']).lower()
            if keyword_lower in ranking_map:
                rank_data = ranking_map[keyword_lower]
                position = rank_data['position']
                
                # Rozszerzony status z pozycjƒÖ
                if position == 0 or pd.isna(position):
                    status = 'Do optymalizacji (Nie rankuje)'
                elif position <= 3:
                    status = f'Do optymalizacji (Poz. {int(position)} ‚Üí TOP1)'
                elif position <= 10:
                    status = f'Do optymalizacji (Poz. {int(position)} ‚Üí TOP3)'
                elif position <= 20:
                    status = f'Do optymalizacji (Poz. {int(position)} ‚Üí TOP10)'
                else:
                    status = f'Do optymalizacji (Poz. {int(position)} ‚Üí TOP20)'
                
                results.append({
                    'S≈Çowo kluczowe': row['Keyword'],
                    'Wolumen': row.get('Volume', 0),
                    'Status': status,
                    'Akcja / Dopasowany URL': rank_data['url'],
                    'Podobie≈Ñstwo': 1.00,
                    'Aktualna_pozycja': position
                })
            else:
                keywords_for_semantic_check.append(row.to_dict())
        
        st.info(f"{len(results)} s≈Ç√≥w zmapowano na podstawie rankingu. Pozosta≈Ço {len(keywords_for_semantic_check)} do analizy semantycznej.")
        
        if keywords_for_semantic_check:
            df_semantic = pd.DataFrame(keywords_for_semantic_check)
            
            # Upewniamy siƒô, ≈ºe kolumna Wolumen istnieje
            if 'Volume' in df_semantic.columns and 'Wolumen' not in df_semantic.columns:
                df_semantic['Wolumen'] = df_semantic['Volume']
            elif 'Wolumen' not in df_semantic.columns:
                df_semantic['Wolumen'] = 0
            
            # Dodajemy kolumnƒô S≈Çowo kluczowe je≈õli jest Keyword
            if 'Keyword' in df_semantic.columns and 'S≈Çowo kluczowe' not in df_semantic.columns:
                df_semantic['S≈Çowo kluczowe'] = df_semantic['Keyword']
            
            st.info("üîÑ Generowanie embedding√≥w dla artyku≈Ç√≥w...")
            
            # Generowanie embedding√≥w
            corpus_embeddings = get_openai_embeddings(
                tuple(df_articles['Title'].tolist()),
                openai_api_key
            )
            
            st.info("üîÑ Generowanie embedding√≥w dla s≈Ç√≥w kluczowych...")
            
            query_embeddings = get_openai_embeddings(
                tuple(df_semantic['Keyword' if 'Keyword' in df_semantic.columns else 'S≈Çowo kluczowe'].tolist()),
                openai_api_key
            )

            if not corpus_embeddings or not query_embeddings:
                st.error("Nie uda≈Ço siƒô wygenerowaƒá wektor√≥w. Sprawd≈∫ klucz API i spr√≥buj ponownie.")
                st.stop()
            
            # Klasteryzacja fraz (je≈õli w≈ÇƒÖczona)
            if enable_clustering:
                st.info("üîÑ Klasteryzujƒô frazy kluczowe...")
                df_semantic = cluster_keywords(df_semantic, query_embeddings, cluster_threshold)
            
            # Analiza semantyczna
            cosine_scores = util.cos_sim(torch.tensor(query_embeddings), torch.tensor(corpus_embeddings))
            
            semantic_records = df_semantic.to_dict('records')
            for row_dict, scores in zip(semantic_records, cosine_scores):
                top_result = torch.topk(scores, k=1)
                score, corpus_idx = top_result.values[0].item(), top_result.indices[0].item()
                closest_article_url = df_articles.iloc[corpus_idx]['URL']
                closest_article_title = df_articles.iloc[corpus_idx]['Title']
                
                # Pobieramy s≈Çowo kluczowe z odpowiedniej kolumny
                keyword = row_dict.get('Keyword', row_dict.get('S≈Çowo kluczowe', ''))
                volume = row_dict.get('Volume', row_dict.get('Wolumen', 0))

                if score > similarity_threshold:
                    status = f'Do optymalizacji (Podobne do: {closest_article_title[:50]}...)'
                    url = closest_article_url
                else:
                    status = 'Nowy temat'
                    url = 'Stw√≥rz nowy artyku≈Ç'
                
                # Wykrywanie intencji
                intent = detect_search_intent(keyword)
                
                result = {
                    'S≈Çowo kluczowe': keyword,
                    'Wolumen': volume,
                    'Status': status,
                    'Akcja / Dopasowany URL': url,
                    'Najbli≈ºszy_artyku≈Ç': closest_article_url,
                    'Najbli≈ºszy_tytu≈Ç': closest_article_title,
                    'Podobie≈Ñstwo': round(score, 2),
                    'Intencja': intent,
                    'Aktualna_pozycja': 0
                }
                
                # Dodanie informacji o klastrze
                if enable_clustering:
                    result['Klaster_ID'] = row_dict.get('Klaster_ID', 0)
                    result['HEAD_Keyword'] = row_dict.get('HEAD_Keyword', row_dict['Keyword'])
                    result['Typ_w_klastrze'] = row_dict.get('Typ_w_klastrze', 'HEAD')
                    result['Liczba_fraz_w_klastrze'] = row_dict.get('Liczba_fraz_w_klastrze', 1)
                
                results.append(result)
        
        df_results = pd.DataFrame(results)
        
        # Obliczanie scoring priorytetu
        df_results['Priorytet_Score'] = df_results.apply(calculate_priority_score, axis=1)
        
        # Generowanie tytu≈Ç√≥w tylko dla HEAD keywords w klastrach
        if enable_clustering:
            df_new_topics = df_results[
                (df_results['Status'] == 'Nowy temat') & 
                (df_results['Typ_w_klastrze'] == 'HEAD')
            ].copy()
        else:
            df_new_topics = df_results[df_results['Status'] == 'Nowy temat'].copy()
        
        if not df_new_topics.empty:
            df_to_process = df_new_topics.sort_values(by='Priorytet_Score', ascending=False).head(num_to_generate)
            st.info(f"Generujƒô propozycje tytu≈Ç√≥w dla {len(df_to_process)} najwa≈ºniejszych nowych temat√≥w...")
            
            df_gap_indexed = df_gap.set_index('Keyword')
            df_to_process['Competitor URL'] = df_to_process['S≈Çowo kluczowe'].map(
                df_gap_indexed.apply(find_first_competitor_url, axis=1)
            )

            progress_bar = st.progress(0, text="Generowanie tytu≈Ç√≥w (GPT-4o)...")
            
            generated_titles_data = []
            for i, (idx, row) in enumerate(df_to_process.iterrows()):
                # Zbieranie powiƒÖzanych fraz z klastra
                related_keywords = ""
                if enable_clustering and row.get('Liczba_fraz_w_klastrze', 1) > 1:
                    related = df_results[
                        (df_results['Klaster_ID'] == row['Klaster_ID']) & 
                        (df_results['Typ_w_klastrze'] == 'RELATED')
                    ]['S≈Çowo kluczowe'].tolist()
                    related_keywords = ", ".join(related[:5])  # Max 5 powiƒÖzanych fraz
                
                titles = generate_titles(
                    openai_api_key,
                    row['S≈Çowo kluczowe'],
                    row['Wolumen'],
                    row.get('Competitor URL', 'Brak'),
                    related_keywords
                )
                generated_titles_data.append({
                    'S≈Çowo kluczowe': row['S≈Çowo kluczowe'],
                    'Propozycja_tematu_1': titles[0],
                    'Propozycja_tematu_2': titles[1],
                    'Propozycja_tematu_3': titles[2]
                })
                progress_bar.progress((i + 1) / len(df_to_process), text=f"Generowanie tytu≈Ç√≥w ({i+1}/{len(df_to_process)})")

            if generated_titles_data:
                df_titles = pd.DataFrame(generated_titles_data)
                df_results = pd.merge(df_results, df_titles, on='S≈Çowo kluczowe', how='left')

        df_results.fillna('-', inplace=True)
        st.success("‚úÖ Analiza zako≈Ñczona!")
        
        # Statystyki
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Nowe tematy", len(df_results[df_results['Status'] == 'Nowy temat']))
        with col2:
            st.metric("Do optymalizacji", len(df_results[df_results['Status'].str.contains('optymalizacji', na=False)]))
        with col3:
            if enable_clustering:
                st.metric("Liczba klastr√≥w", df_results['Klaster_ID'].nunique())
        with col4:
            st.metric("≈öredni priorytet", f"{df_results['Priorytet_Score'].mean():.1f}")
        
        st.header("üìä Wyniki Analizy i Plan Tre≈õci")
        
        # Sortowanie wynik√≥w
        df_results_sorted = df_results.sort_values(by=['Priorytet_Score', 'Wolumen'], ascending=[False, False])
        
        # Definicja kolejno≈õci kolumn
        cols_order = [
            'S≈Çowo kluczowe', 'Wolumen', 'Priorytet_Score', 'Status', 
            'Akcja / Dopasowany URL', 'Najbli≈ºszy_artyku≈Ç', 'Podobie≈Ñstwo',
            'Intencja', 'Aktualna_pozycja'
        ]
        
        if enable_clustering:
            cols_order.extend(['Klaster_ID', 'HEAD_Keyword', 'Typ_w_klastrze', 'Liczba_fraz_w_klastrze'])
        
        cols_order.extend(['Propozycja_tematu_1', 'Propozycja_tematu_2', 'Propozycja_tematu_3'])
        
        existing_cols = [col for col in cols_order if col in df_results_sorted.columns]
        
        st.dataframe(df_results_sorted[existing_cols], use_container_width=True)
        
        # Export do CSV
        csv_buffer = io.StringIO()
        df_results_sorted[existing_cols].to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_bytes = csv_buffer.getvalue().encode('utf-8-sig')

        st.download_button(
            "üì• Pobierz gotowy plan tre≈õci jako CSV", 
            csv_bytes, 
            "plan_tresci_ultimate.csv", 
            "text/csv",
            type="primary"
        )
