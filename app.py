import streamlit as st
import pandas as pd
import openai
from sentence_transformers import SentenceTransformer, util
import torch
import io
import re

# Ustawienia strony Streamlit
st.set_page_config(page_title="Planer Treci SEO", layout="wide")

# --- Funkcje pomocnicze ---

@st.cache_resource
def load_model():
    """aduje i cachuje model SentenceTransformer."""
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def find_first_competitor_url(row):
    """Znajduje pierwszy dostpny URL konkurenta w danym wierszu."""
    for col in row.index:
        if col.endswith(': URL') and pd.notna(row[col]):
            return row[col]
    return "Brak"

def generate_titles(api_key, keyword, volume, competitor_url):
    """Generuje tytuy za pomoc API OpenAI."""
    try:
        client = openai.OpenAI(api_key=api_key)
        
        prompt = f"""
        Jeste wiatowej klasy strategiem contentu SEO i copywriterem. Twoim zadaniem jest tworzenie propozycji tytu贸w artyku贸w blogowych, kt贸re maj ogromn szans na zdobycie wysokich pozycji w Google i przycignicie uwagi czytelnika.

        Przeanalizuj poni偶sze dane:
        - Sowo kluczowe do targetowania: "{keyword}"
        - Miesiczny wolumen wyszukiwania: {volume}
        - Przykadowy artyku konkurencji, kt贸ry ju偶 jest w TOP10: {competitor_url}

        Twoje zadanie:
        Zaproponuj 3 unikalne, anga偶ujce i zoptymalizowane pod SEO tytuy artyku贸w blogowych. Skup si na intencji u偶ytkownika, kt贸ra jest g贸wnie informacyjna. Tytuy powinny by w formie listy numerowanej (1. Tytu 1, 2. Tytu 2, 3. Tytu 3). Bd藕 kreatywny i unikaj prostego powtarzania sowa kluczowego.
        """
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Jeste ekspertem SEO i copywriterem."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=150
        )
        
        content = response.choices[0].message.content
        titles = re.findall(r'\d+\.\s*(.*)', content)
        while len(titles) < 3:
            titles.append("---")
        return titles[:3]

    except Exception as e:
        st.error(f"Bd podczas generowania tytu贸w dla '{keyword}': {e}")
        return ["Bd API", "Bd API", "Bd API"]

# --- Interfejs U偶ytkownika (UI) ---

st.title(" Planer Treci SEO oparty o Analiz Content Gap")
st.markdown("Aplikacja do automatycznego planowania treci blogowych. Wgraj plik z analiz luki w treci oraz list swoich artyku贸w, a aplikacja zidentyfikuje istniejce treci i wygeneruje propozycje nowych.")

# Kolumny dla lepszego ukadu
col1, col2 = st.columns(2)

with col1:
    st.header("1. Konfiguracja")
    num_to_generate = st.number_input(
        "Liczba nowych artyku贸w do wygenerowania", 
        min_value=1, value=20,
        help="Okrel, dla ilu najwa偶niejszych s贸w kluczowych (z najwikszym wolumenem) chcesz wygenerowa propozycje tytu贸w."
    )
    similarity_threshold = st.slider(
        "Pr贸g podobiestwa semantycznego", 
        min_value=0.5, max_value=1.0, value=0.75, step=0.01,
        help="Jak bardzo podobne musi by sowo kluczowe do tytuu artykuu, aby uzna temat za pokryty? (domylnie: 0.75)"
    )

with col2:
    st.header("2. Wgraj pliki CSV")
    content_gap_file = st.file_uploader("Wgraj plik CSV z analiz Content Gap", type="csv")
    my_articles_file = st.file_uploader("Wgraj plik CSV z list swoich artyku贸w", type="csv")

# --- Logika Aplikacji ---

if st.button("Uruchom Analiz", type="primary"):
    openai_api_key = st.secrets.get("OPENAI_API_KEY")
    if not openai_api_key:
        st.error("Klucz API OpenAI nie zosta znaleziony w sekretach Streamlit! Upewnij si, 偶e dodae go w ustawieniach aplikacji.")
    elif content_gap_file is None or my_articles_file is None:
        st.warning("Upewnij si, 偶e wgrae oba pliki CSV.")
    else:
        with st.spinner("Przeprowadzam analiz... To mo偶e potrwa kilka minut."):
            try:
                df_gap = pd.read_csv(content_gap_file)
                df_articles = pd.read_csv(my_articles_file)
                
                df_articles.rename(columns={'Address': 'URL', 'Title 1': 'Title'}, inplace=True)
                df_articles = df_articles[~df_articles['Title'].str.contains("Bot Verification|Strona|Kategoria", na=False, case=False)]
                df_articles.dropna(subset=['Title', 'URL'], inplace=True)
                
                st.info(f"Znaleziono {len(df_gap)} s贸w kluczowych w analizie content gap.")
                st.info(f"Znaleziono {len(df_articles)} unikalnych artyku贸w do analizy.")
            except Exception as e:
                st.error(f"Bd podczas wczytywania plik贸w CSV: {e}")
                st.stop()

            model = load_model()
            
            corpus_embeddings = model.encode(df_articles['Title'].tolist(), convert_to_tensor=True, show_progress_bar=True)
            query_embeddings = model.encode(df_gap['Keyword'].tolist(), convert_to_tensor=True, show_progress_bar=True)
            
            hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=1)
            
            results = []
            for i, row in df_gap.iterrows():
                keyword = row['Keyword']
                volume = row['Volume']
                best_hit = hits[i][0]
                
                if best_hit['score'] > similarity_threshold:
                    matched_article_url = df_articles.iloc[best_hit['corpus_id']]['URL']
                    results.append({'Sowo kluczowe': keyword, 'Wolumen': volume, 'Status': 'Ju偶 istnieje', 'Akcja / Dopasowany URL': matched_article_url, 'Podobiestwo': round(best_hit['score'], 2)})
                else:
                    results.append({'Sowo kluczowe': keyword, 'Wolumen': volume, 'Status': 'Nowy temat', 'Akcja / Dopasowany URL': 'Stw贸rz nowy artyku', 'Podobiestwo': round(best_hit['score'], 2)})
            
            df_results = pd.DataFrame(results)
            
            df_new_topics = df_results[df_results['Status'] == 'Nowy temat'].copy()
            
            if not df_new_topics.empty:
                df_new_topics_sorted = df_new_topics.sort_values(by='Wolumen', ascending=False)
                df_to_process = df_new_topics_sorted.head(num_to_generate)
                
                st.info(f"Znaleziono {len(df_new_topics)} nowych temat贸w. Rozpoczynam generowanie propozycji dla {len(df_to_process)} najwa偶niejszych z nich...")
                
                original_data_for_processing = df_gap[df_gap['Keyword'].isin(df_to_process['Sowo kluczowe'])]
                df_to_process['Competitor URL'] = original_data_for_processing.apply(find_first_competitor_url, axis=1).values
                
                progress_bar = st.progress(0, text="Generowanie tytu贸w...")
                total_to_process = len(df_to_process)
                
                generated_titles = []
                for i, (_, row) in enumerate(df_to_process.iterrows()):
                    titles = generate_titles(openai_api_key, row['Sowo kluczowe'], row['Wolumen'], row['Competitor URL'])
                    generated_titles.append(titles)
                    progress_bar.progress((i + 1) / total_to_process, text=f"Generowanie tytu贸w... ({i+1}/{total_to_process})")

                df_titles = pd.DataFrame(generated_titles, columns=['Propozycja tematu 1', 'Propozycja tematu 2', 'Propozycja tematu 3'], index=df_to_process.index)
                df_results = df_results.join(df_titles)
            
            df_results.fillna('-', inplace=True)
            st.success("Analiza zakoczona!")

            st.header("Wyniki Analizy i Plan Treci")
            
            df_results_sorted = df_results.sort_values(by=['Status', 'Wolumen'], ascending=[True, False])
            st.dataframe(df_results_sorted)
            
            csv_buffer = io.StringIO()
            df_results_sorted.to_csv(csv_buffer, index=False, encoding='utf-8')
            csv_bytes = csv_buffer.getvalue().encode('utf-8')

            st.download_button(
                label="Pobierz gotowy plan treci jako CSV",
                data=csv_bytes,
                file_name="plan_tresci.csv",
                mime="text/csv",
            )
