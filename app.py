import streamlit as st
import pandas as pd
import openai
from sentence_transformers import util
import torch
import io
import re

# Ustawienia strony Streamlit
st.set_page_config(page_title="Planer Tre≈õci SEO", layout="wide")

# --- Funkcje pomocnicze ---

@st.cache_data(show_spinner=False)
def get_openai_embeddings(_texts, api_key):
    """Generuje wektory (embeddings) za pomocƒÖ API OpenAI, czyszczƒÖc dane wej≈õciowe."""
    client = openai.OpenAI(api_key=api_key)
    clean_texts = [str(text).strip() if pd.notna(text) and str(text).strip() else " " for text in _texts]

    try:
        response = client.embeddings.create(
            input=clean_texts,
            model="text-embedding-3-large"  # ZMIANA MODELU EMBEDDINGOWEGO
        )
        return [item.embedding for item in response.data]
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas generowania wektor√≥w OpenAI: {e}")
        return None

def find_first_competitor_url(row):
    """Znajduje pierwszy dostƒôpny URL konkurenta w danym wierszu."""
    for col in row.index:
        if isinstance(col, str) and col.endswith(': URL') and pd.notna(row[col]):
            return row[col]
    return "Brak"

def generate_titles(api_key, keyword, volume, competitor_url):
    """Generuje tytu≈Çy za pomocƒÖ API OpenAI (GPT-4 Turbo)."""
    try:
        client = openai.OpenAI(api_key=api_key)
        prompt = f"""
        Jeste≈õ ekspertem SEO i copywriterem specjalizujƒÖcym siƒô w tworzeniu anga≈ºujƒÖcych tytu≈Ç√≥w na polskojƒôzyczne blogi.

        Przeanalizuj poni≈ºsze dane:
        - S≈Çowo kluczowe do u≈ºycia: "{keyword}"
        - Miesiƒôczny wolumen wyszukiwania: {volume}
        - Artyku≈Ç konkurencji: {competitor_url}

        Twoje zadanie:
        Zaproponuj 3 unikalne tytu≈Çy artyku≈Ç√≥w blogowych.

        Zasady, kt√≥rych musisz bezwzglƒôdnie przestrzegaƒá:
        1. Ka≈ºdy tytu≈Ç musi zawieraƒá dok≈ÇadnƒÖ frazƒô kluczowƒÖ: "{keyword}".
        2. Tytu≈Çy muszƒÖ mieƒá charakter informacyjny lub poradnikowy (np. "Jak...", "Co to jest...", "Przewodnik po...").
        3. Stosuj polskie zasady pisowni ‚Äì tylko pierwsza litera w tytule wielka (reszta ma≈Çymi, chyba ≈ºe to nazwa w≈Çasna).
        4. Zamiast dwukropka u≈ºywaj my≈õlnika (np. "Tytu≈Ç ‚Äì podtyku≈Ç").
        5. Zwr√≥ƒá odpowied≈∫ wy≈ÇƒÖcznie w formie listy numerowanej (1. Tytu≈Ç, 2. Tytu≈Ç, 3. Tytu≈Ç), bez ≈ºadnych dodatkowych wstƒôp√≥w ani wyja≈õnie≈Ñ.
        """
        
        response = client.chat.completions.create(
            model="gpt-4-turbo", # ZMIANA MODELU CHATU NA NAJLEPSZY DOSTƒòPNY
            messages=[
                {"role": "system", "content": "Jeste≈õ ekspertem SEO i copywriterem."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
            # Usuniƒôto parametr max_tokens dla maksymalnej kompatybilno≈õci
        )
        
        content = response.choices[0].message.content
        titles = re.findall(r'\d+\.\s*(.*)', content)
        while len(titles) < 3:
            titles.append("---")
        return titles[:3]

    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas generowania tytu≈Ç√≥w dla '{keyword}': {e}")
        return ["B≈ÇƒÖd API", "B≈ÇƒÖd API", "B≈ÇƒÖd API"]

# --- Interfejs U≈ºytkownika (UI) ---
st.title("üöÄ Planer Tre≈õci SEO [Wersja Hybrydowa v4 - PRO]")
st.markdown("Najnowsza wersja z precyzyjnym mapowaniem ranking√≥w i zaawansowanymi modelami AI od OpenAI.")

col1, col2 = st.columns(2)

with col1:
    st.header("1. Konfiguracja")
    num_to_generate = st.number_input("Liczba nowych artyku≈Ç√≥w do wygenerowania", min_value=1, value=20, help="Okre≈õl, dla ilu najwa≈ºniejszych s≈Ç√≥w kluczowych chcesz wygenerowaƒá propozycje tytu≈Ç√≥w.")
    similarity_threshold = st.slider("Pr√≥g podobie≈Ñstwa dla optymalizacji", min_value=0.7, max_value=1.0, value=0.8, step=0.01, help="Pr√≥g, powy≈ºej kt√≥rego artyku≈Ç zostanie uznany za 'Do optymalizacji' (domy≈õlnie: 0.80)")

with col2:
    st.header("2. Wgraj pliki CSV")
    content_gap_file = st.file_uploader("1. Wgraj plik CSV z analizƒÖ Content Gap", type="csv")
    my_articles_file = st.file_uploader("2. Wgraj plik CSV z listƒÖ swoich artyku≈Ç√≥w", type="csv")
    ranking_file = st.file_uploader("3. Wgraj plik CSV z aktualnym rankingiem", type="csv")

# --- Logika Aplikacji ---
if st.button("Uruchom Analizƒô HybrydowƒÖ", type="primary"):
    openai_api_key = st.secrets.get("OPENAI_API_KEY")
    if not openai_api_key:
        st.error("Klucz API OpenAI nie zosta≈Ç znaleziony w sekretach Streamlit!")
    elif not all([content_gap_file, my_articles_file, ranking_file]):
        st.warning("Upewnij siƒô, ≈ºe wgra≈Çe≈õ wszystkie trzy pliki CSV.")
    else:
        with st.spinner("Przeprowadzam analizƒô... To mo≈ºe potrwaƒá kilka minut."):
            try:
                # Wczytywanie i rygorystyczne czyszczenie danych
                df_gap = pd.read_csv(content_gap_file).dropna(subset=['Keyword']).astype({'Keyword': str})
                df_articles = pd.read_csv(my_articles_file)
                df_ranking = pd.read_csv(ranking_file)
                
                df_articles.rename(columns={'Address': 'URL', 'Title 1': 'Title'}, inplace=True)
                df_articles.dropna(subset=['Title', 'URL'], inplace=True)
                df_articles = df_articles[df_articles['Title'].str.strip() != '']
                df_articles = df_articles[~df_articles['Title'].str.contains("Bot Verification|Strona|Kategoria", na=False, case=False)]
                df_ranking.dropna(subset=['S≈Çowo kluczowe', 'Adres URL'], inplace=True)
                
                st.info(f"Wczytano {len(df_gap)} s≈Ç√≥w kluczowych, {len(df_articles)} artyku≈Ç√≥w i {len(df_ranking)} rankingowych s≈Ç√≥w kluczowych.")
            except Exception as e:
                st.error(f"B≈ÇƒÖd podczas wczytywania plik√≥w CSV: {e}")
                st.stop()

            # Krok 1: Mapowanie na podstawie rankingu
            ranking_keywords = set(df_ranking['S≈Çowo kluczowe'].str.lower())
            ranking_map = df_ranking.set_index(df_ranking['S≈Çowo kluczowe'].str.lower())['Adres URL'].to_dict()
            
            results = []
            keywords_for_semantic_check = []
            
            for _, row in df_gap.iterrows():
                keyword_lower = str(row['Keyword']).lower()
                if keyword_lower in ranking_keywords:
                    results.append({'S≈Çowo kluczowe': row['Keyword'], 'Wolumen': row.get('Volume', 0), 'Status': 'Ju≈º istnieje', 'Akcja / Dopasowany URL': ranking_map[keyword_lower], 'Podobie≈Ñstwo': 1.00})
                else:
                    keywords_for_semantic_check.append(row.to_dict())

            st.info(f"{len(results)} s≈Ç√≥w zmapowano na podstawie rankingu. Pozosta≈Ço {len(keywords_for_semantic_check)} do analizy semantycznej.")

            # Krok 2: Analiza semantyczna dla pozosta≈Çych
            if keywords_for_semantic_check:
                df_semantic = pd.DataFrame(keywords_for_semantic_check)
                st.info("Generowanie wektor√≥w przez API OpenAI (text-embedding-3-large)...")
                
                corpus_embeddings_openai = get_openai_embeddings(df_articles['Title'].tolist(), openai_api_key)
                query_embeddings_openai = get_openai_embeddings(df_semantic['Keyword'].tolist(), openai_api_key)

                if corpus_embeddings_openai and query_embeddings_openai:
                    corpus_tensor = torch.tensor(corpus_embeddings_openai)
                    query_tensor = torch.tensor(query_embeddings_openai)
                    hits = util.semantic_search(query_tensor, corpus_tensor, top_k=1)
                    
                    # --- POPRAWIONA I BEZPIECZNA PƒòTLA ---
                    semantic_records = df_semantic.to_dict('records')
                    for row_dict, hit_list in zip(semantic_records, hits):
                        if hit_list:
                            best_hit = hit_list[0]
                            if best_hit['score'] > similarity_threshold:
                                status = 'Do optymalizacji'
                                url = df_articles.iloc[best_hit['corpus_id']]['URL']
                                score = round(best_hit['score'], 2)
                            else:
                                status = 'Nowy temat'
                                url = 'Stw√≥rz nowy artyku≈Ç'
                                score = round(best_hit['score'], 2)
                        else:
                            status = 'Nowy temat'
                            url = 'Stw√≥rz nowy artyku≈Ç'
                            score = 0.0

                        results.append({'S≈Çowo kluczowe': row_dict['Keyword'], 'Wolumen': row_dict.get('Volume', 0), 'Status': status, 'Akcja / Dopasowany URL': url, 'Podobie≈Ñstwo': score})
            
            df_results = pd.DataFrame(results)
            
            # Krok 3: Generowanie tytu≈Ç√≥w
            df_new_topics = df_results[df_results['Status'] == 'Nowy temat'].copy()
            if not df_new_topics.empty:
                df_to_process = df_new_topics.sort_values(by='Wolumen', ascending=False).head(num_to_generate)
                st.info(f"Generujƒô propozycje tytu≈Ç√≥w dla {len(df_to_process)} najwa≈ºniejszych nowych temat√≥w...")
                
                original_data = df_gap.set_index('Keyword')
                urls_to_add = original_data.apply(find_first_competitor_url, axis=1)
                df_to_process = df_to_process.merge(urls_to_add.rename('Competitor URL'), left_on='S≈Çowo kluczowe', right_index=True, how='left')

                progress_bar = st.progress(0, text="Generowanie tytu≈Ç√≥w (GPT-4)...")
                
                generated_titles_list = []
                for i, row in enumerate(df_to_process.itertuples()):
                    titles = generate_titles(openai_api_key, row._1, row.Wolumen, row._6)
                    generated_titles_list.append(titles)
                    progress_bar.progress((i + 1) / len(df_to_process), text=f"Generowanie tytu≈Ç√≥w ({i+1}/{len(df_to_process)})")

                df_titles = pd.DataFrame(generated_titles_list, columns=['Propozycja tematu 1', 'Propozycja tematu 2', 'Propozycja tematu 3'], index=df_to_process.index)
                df_results = df_results.merge(df_titles, how='left', left_index=True, right_index=True)

            df_results.fillna('-', inplace=True)
            st.success("Analiza zako≈Ñczona!")
            st.header("Wyniki Analizy i Plan Tre≈õci")
            
            cols_order = ['S≈Çowo kluczowe', 'Wolumen', 'Status', 'Akcja / Dopasowany URL', 'Propozycja tematu 1', 'Propozycja tematu 2', 'Propozycja tematu 3', 'Podobie≈Ñstwo']
            existing_cols = [col for col in cols_order if col in df_results.columns]
            df_results_sorted = df_results.sort_values(by=['Status', 'Wolumen'], ascending=[True, False])
            
            st.dataframe(df_results_sorted[existing_cols])
            
            csv_buffer = io.StringIO()
            df_results_sorted[existing_cols].to_csv(csv_buffer, index=False, encoding='utf-8')
            csv_bytes = csv_buffer.getvalue().encode('utf-8-sig')

            st.download_button("Pobierz gotowy plan tre≈õci jako CSV", csv_bytes, "plan_tresci.csv", "text/csv")
