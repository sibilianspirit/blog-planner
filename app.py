import streamlit as st
import pandas as pd
import openai
from sentence_transformers import util
import torch
import io
import re
import time
import math
import hdbscan
import numpy as np
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import json

# Ustawienia strony Streamlit
st.set_page_config(page_title="Planer TreÅ›ci SEO", layout="wide")

# Inicjalizacja session_state do przechowywania wynikÃ³w analizy
# To jest kluczowy dodatek, ktÃ³ry zapobiega utracie danych po klikniÄ™ciu przycisku
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None

# --- Funkcje pomocnicze ---

@st.cache_data(show_spinner=False)
def get_openai_embeddings(texts_tuple, api_key, batch_size=256):
    """
    Generuje wektory (embeddings) za pomocÄ… API OpenAI, uÅ¼ywajÄ…c batchingu
    dla obsÅ‚ugi duÅ¼ych list i unikniÄ™cia bÅ‚Ä™dÃ³w API.
    """
    texts = list(texts_tuple)
    client = openai.OpenAI(api_key=api_key)
    all_embeddings = []
    
    clean_texts = [str(text).strip() if pd.notna(text) and str(text).strip() else " " for text in texts]

    num_batches = math.ceil(len(clean_texts) / batch_size)
    
    for i in range(num_batches):
        start_index = i * batch_size
        end_index = start_index + batch_size
        batch = clean_texts[start_index:end_index]
        
        try:
            response = client.embeddings.create(input=batch, model="text-embedding-3-large")
            all_embeddings.extend([item.embedding for item in response.data])
            time.sleep(1)

        except Exception as e:
            st.error(f"BÅ‚Ä…d podczas przetwarzania batcha nr {i+1}: {e}")
            return None
            
    return all_embeddings

def cluster_keywords_hdbscan(keywords_df, embeddings, min_cluster_size=2, min_samples=1):
    """
    Klasteryzuje frazy kluczowe uÅ¼ywajÄ…c algorytmu HDBSCAN.
    HDBSCAN automatycznie znajduje optymalnÄ… liczbÄ™ klastrÃ³w i wykrywa outliery.
    
    Args:
        keywords_df: DataFrame ze sÅ‚owami kluczowymi
        embeddings: Lista embeddingÃ³w
        min_cluster_size: Minimalna wielkoÅ›Ä‡ klastra (domyÅ›lnie 2)
        min_samples: Minimalna liczba prÃ³bek w sÄ…siedztwie (domyÅ›lnie 1)
    
    Returns:
        DataFrame z przypisanymi klastrami i HEAD keywords
    """
    if len(keywords_df) == 0:
        return keywords_df
    
    # Upewniamy siÄ™, Å¼e kolumna Wolumen istnieje
    if 'Volume' not in keywords_df.columns and 'Wolumen' not in keywords_df.columns:
        keywords_df['Wolumen'] = 0
    elif 'Volume' in keywords_df.columns and 'Wolumen' not in keywords_df.columns:
        keywords_df['Wolumen'] = keywords_df['Volume']
    
    # Konwersja do numpy array
    embeddings_array = np.array(embeddings)
    
    # HDBSCAN klasteryzacja
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric='euclidean',
        cluster_selection_method='eom',  # Excess of Mass - bardziej konserwatywne
        prediction_data=True
    )
    
    cluster_labels = clusterer.fit_predict(embeddings_array)
    
    # Dodanie informacji o klastrach do DataFrame
    keywords_df['Klaster_ID'] = cluster_labels
    
    # -1 oznacza outlier (frazy, ktÃ³re nie pasujÄ… do Å¼adnego klastra)
    # Przypisujemy im unikalne ID klastra
    outlier_mask = keywords_df['Klaster_ID'] == -1
    if outlier_mask.any():
        max_cluster_id = keywords_df['Klaster_ID'].max()
        unique_outlier_ids = range(max_cluster_id + 1, max_cluster_id + 1 + outlier_mask.sum())
        keywords_df.loc[outlier_mask, 'Klaster_ID'] = list(unique_outlier_ids)
        keywords_df.loc[outlier_mask, 'Jest_Outlier'] = True
    
    keywords_df['Jest_Outlier'] = keywords_df.get('Jest_Outlier', False)
    
    # Dodanie informacji o sile przynaleÅ¼noÅ›ci do klastra
    probabilities = clusterer.probabilities_ if hasattr(clusterer, 'probabilities_') else np.ones(len(keywords_df))
    keywords_df['Cluster_Probability'] = probabilities
    
    # Wyznaczenie HEAD keyword dla kaÅ¼dego klastra
    def get_head_keyword(group):
        if len(group) == 0:
            return group
            
        keyword_col = 'Keyword' if 'Keyword' in group.columns else 'SÅ‚owo kluczowe'
        
        # Dla outlierÃ³w (pojedyncze frazy) - sÄ… same HEAD
        if len(group) == 1:
            group['HEAD_Keyword'] = group[keyword_col].iloc[0]
            group['Typ_w_klastrze'] = 'HEAD'
            group['Liczba_fraz_w_klastrze'] = 1
            return group
        
        # Dla normalnych klastrÃ³w - wybieramy HEAD na podstawie wolumenu
        if 'Wolumen' in group.columns:
            # Sortujemy po wolumenie * probability (preferujemy frazy z wysokÄ… pewnoÅ›ciÄ…)
            group['_score'] = group['Wolumen'] * (group['Cluster_Probability'] + 0.1)
            head_idx = group['_score'].idxmax()
            group.drop('_score', axis=1, inplace=True)
            
            group['HEAD_Keyword'] = group.loc[head_idx, keyword_col]
            group['Typ_w_klastrze'] = 'RELATED'
            group.loc[head_idx, 'Typ_w_klastrze'] = 'HEAD'
            group['Liczba_fraz_w_klastrze'] = len(group)
            
        return group
    
    # UÅ¼ycie .apply() bez dodatkowych, przestarzaÅ‚ych argumentÃ³w
    keywords_df = keywords_df.groupby('Klaster_ID').apply(get_head_keyword).reset_index(drop=True)
    
    # Dodanie informacji o jakoÅ›ci klastra
    def calculate_cluster_quality(group):
        # --- POCZÄ„TEK POPRAWKI ---
        if len(group) <= 1:
            group['Cluster_Quality'] = 1.0  # Klaster z 1 elementem ma idealnÄ… jakoÅ›Ä‡
            return group
        # --- KONIEC POPRAWKI ---
            
        # Åšrednia probability w klastrze
        group['Cluster_Quality'] = group['Cluster_Probability'].mean()
        return group
    
    # UÅ¼ycie .apply() bez dodatkowych, przestarzaÅ‚ych argumentÃ³w
    keywords_df = keywords_df.groupby('Klaster_ID').apply(calculate_cluster_quality).reset_index(drop=True)
    
    # Upewnienie siÄ™, Å¼e kolumna istnieje, na wypadek gdyby coÅ› poszÅ‚o nie tak
    if 'Cluster_Quality' not in keywords_df.columns:
        keywords_df['Cluster_Quality'] = 1.0
    
    return keywords_df

def analyze_cluster_coherence(keywords_df, embeddings):
    """
    Analizuje spÃ³jnoÅ›Ä‡ semantycznÄ… klastrÃ³w.
    Zwraca statystyki pomocne w ocenie jakoÅ›ci klasteryzacji.
    """
    if 'Klaster_ID' not in keywords_df.columns:
        return {}
    
    embeddings_array = np.array(embeddings)
    stats = {
        'total_clusters': keywords_df['Klaster_ID'].nunique(),
        'outliers': keywords_df['Jest_Outlier'].sum(),
        'avg_cluster_size': keywords_df.groupby('Klaster_ID').size().mean(),
        'max_cluster_size': keywords_df.groupby('Klaster_ID').size().max(),
        'clusters_details': []
    }
    
    # Analiza kaÅ¼dego klastra
    for cluster_id in keywords_df['Klaster_ID'].unique():
        cluster_data = keywords_df[keywords_df['Klaster_ID'] == cluster_id]
        if len(cluster_data) <= 1:
            continue
            
        # Pobieramy embeddingi dla tego klastra
        cluster_indices = cluster_data.index.tolist()
        cluster_embeddings = embeddings_array[cluster_indices]
        
        # Obliczamy Å›rednie podobieÅ„stwo wewnÄ…trz klastra
        similarity_matrix = util.cos_sim(
            torch.tensor(cluster_embeddings),
            torch.tensor(cluster_embeddings)
        ).numpy()
        
        # Usuwamy diagonalÄ™ (podobieÅ„stwo do samego siebie)
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        avg_similarity = similarity_matrix[mask].mean()
        
        keyword_col = 'Keyword' if 'Keyword' in cluster_data.columns else 'SÅ‚owo kluczowe'
        
        stats['clusters_details'].append({
            'cluster_id': cluster_id,
            'size': len(cluster_data),
            'avg_similarity': round(avg_similarity, 3),
            'keywords': cluster_data[keyword_col].tolist()[:5],  # Pierwsze 5 fraz
            'total_volume': cluster_data['Wolumen'].sum()
        })
    
    return stats

def detect_search_intent(keyword):
    """
    Wykrywa intencjÄ™ wyszukiwania na podstawie wzorcÃ³w w frazie kluczowej.
    """
    keyword_lower = keyword.lower()
    
    # Wzorce informacyjne
    info_patterns = ['jak', 'co to', 'dlaczego', 'kiedy', 'gdzie', 'czy', 'jakie', 'rÃ³Å¼nica', 'czym jest']
    # Wzorce transakcyjne
    trans_patterns = ['kup', 'cena', 'sklep', 'oferta', 'promocja', 'tani', 'najleps', 'gdzie kupiÄ‡']
    # Wzorce nawigacyjne
    nav_patterns = ['logowanie', 'kontakt', 'strona', 'oficjalna']
    
    if any(pattern in keyword_lower for pattern in trans_patterns):
        return 'Transakcyjna'
    elif any(pattern in keyword_lower for pattern in nav_patterns):
        return 'Nawigacyjna'
    elif any(pattern in keyword_lower for pattern in info_patterns):
        return 'Informacyjna'
    else:
        return 'Mieszana'

def calculate_priority_score(row):
    """
    Oblicza scoring priorytetu dla danej frazy kluczowej.
    WzÃ³r: (Wolumen Ã— 0.4) + (Pozycja Ã— 0.3) + (PodobieÅ„stwo Ã— 0.2) + (Cluster_Quality Ã— 0.1)
    """
    volume_score = min(row.get('Wolumen', 0) / 1000, 100)  # Normalizacja do 100
    
    # Scoring pozycji (im gorsza pozycja, tym wiÄ™kszy potencjaÅ‚)
    position = row.get('Aktualna_pozycja', 0)
    if position == 0:  # Nie rankuje
        position_score = 100
    elif position > 20:
        position_score = 80
    elif position > 10:
        position_score = 60
    elif position > 5:
        position_score = 40
    else:
        position_score = 20
    
    # Scoring podobieÅ„stwa (niÅ¼sze = nowy temat = wyÅ¼szy priorytet)
    similarity_score = (1 - row.get('PodobieÅ„stwo', 0)) * 100
    
    # Bonus za wysokÄ… jakoÅ›Ä‡ klastra (wysoka coherence)
    cluster_quality_score = row.get('Cluster_Quality', 1.0) * 100
    
    priority = (volume_score * 0.4) + (position_score * 0.3) + (similarity_score * 0.2) + (cluster_quality_score * 0.1)
    return round(priority, 2)

def find_first_competitor_url(row):
    """Znajduje pierwszy dostÄ™pny URL konkurenta w danym wierszu."""
    for col in row.index:
        if isinstance(col, str) and col.endswith(': URL') and pd.notna(row[col]):
            return row[col]
    return "Brak"

def generate_titles(api_key, keyword, volume, competitor_url, related_keywords="", max_retries=3):
    """Generuje tytuÅ‚y za pomocÄ… API OpenAI (GPT-4o)."""
    client = openai.OpenAI(api_key=api_key)
    
    related_info = f"\n- PowiÄ…zane frazy do uwzglÄ™dnienia: {related_keywords}" if related_keywords else ""
    
    prompt = f"""
JesteÅ› ekspertem SEO i copywriterem specjalizujÄ…cym siÄ™ w tworzeniu angaÅ¼ujÄ…cych tytuÅ‚Ã³w na polskojÄ™zyczne blogi sportowe i fitness.
Przeanalizuj poniÅ¼sze dane:
- GÅ‚Ã³wne sÅ‚owo kluczowe: "{keyword}"
- MiesiÄ™czny wolumen wyszukiwania: {volume}{related_info}
- ArtykuÅ‚ konkurencji: {competitor_url}

Twoje zadanie: Zaproponuj 3 unikalne tytuÅ‚y artykuÅ‚Ã³w blogowych.

ZASADY OBOWIÄ„ZKOWE:
1. Frazy kluczowe odmieÅ„ i uÅ¼yj naturalnie - NIE kopiuj dosÅ‚ownie:
   âŒ Å¹LE: "venum dres - jaki model wybraÄ‡"
   âœ… DOBRZE: "Dres Venum - jaki model wybraÄ‡"
   âŒ Å¹LE: "karate szkoÅ‚a jakÄ… wybraÄ‡"
   âœ… DOBRZE: "JakÄ… wybraÄ‡ szkoÅ‚Ä™ karate"

2. Rozpoznawaj nazwy marek i traktuj je jako proper names (pisz wielkÄ… literÄ…):
   - Venum, Manto, Adidas, Nike to marki sportowe
   - NIE pisz "Co to jest manto dres" (kaÅ¼dy wie co to dres)
   - PISZ "Manto - dlaczego warto wybraÄ‡ ubrania od tego producenta"
   - PISZ "Dres Manto - wszystko co musisz wiedzieÄ‡ przed zakupem"

3. JeÅ›li fraza sugeruje porÃ³wnanie/ranking, JEDEN tytuÅ‚ musi byÄ‡ rankingowy:
   - "rÄ™kawice bokserskie" â†’ "TOP 10 rÄ™kawic bokserskich - ranking 2025"
   - "ochraniacze na piszczele" â†’ "Najlepsze ochraniacze na piszczele - ranking i porÃ³wnanie"
   - UÅ¼ywaj formatÃ³w: TOP 10, ranking, najlepsze, porÃ³wnanie

4. Typy tytuÅ‚Ã³w do wykorzystania (zrÃ³Å¼nicuj 3 propozycje):
   - Poradnikowy: "Jak wybraÄ‡...", "Na co zwrÃ³ciÄ‡ uwagÄ™ przy..."
   - Rankingowy: "TOP 10...", "Najlepsze...", "Ranking..."
   - Problemowy: "Dlaczego...", "Co musisz wiedzieÄ‡ o..."
   - Ekspercki: "Przewodnik po...", "Wszystko o..."

5. Stosuj polskie zasady pisowni:
   - Tylko pierwsza litera wielka (poza nazwami wÅ‚asnymi)
   - Zamiast dwukropka uÅ¼ywaj myÅ›lnika
   - Naturalny, pÅ‚ynny jÄ™zyk polski

6. ZwrÃ³Ä‡ odpowiedÅº WYÅÄ„CZNIE w formie listy numerowanej (bez dodatkowych komentarzy).

PrzykÅ‚ady DOBRYCH tytuÅ‚Ã³w:
- "RÄ™kawice bokserskie Venum - jak wybraÄ‡ odpowiedni model dla siebie"
- "TOP 10 najlepszych dresÃ³w do MMA - ranking 2025"
- "JakÄ… wybraÄ‡ szkoÅ‚Ä™ karate - kompletny przewodnik dla poczÄ…tkujÄ…cych"
"""
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "JesteÅ› ekspertem SEO i copywriterem."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=400
            )
            content = response.choices[0].message.content
            titles = re.findall(r'\d+\.\s*(.*)', content)
            
            while len(titles) < 3:
                titles.append("---")
            
            time.sleep(0.5)
            return titles[:3]
            
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                time.sleep(wait_time)
                continue
            else:
                st.warning(f"BÅ‚Ä…d podczas generowania tytuÅ‚Ã³w dla '{keyword}': {e}")
                return ["BÅ‚Ä…d API", "BÅ‚Ä…d API", "BÅ‚Ä…d API"]

def validate_api_key(api_key):
    """Waliduje klucz API OpenAI."""
    try:
        client = openai.OpenAI(api_key=api_key)
        client.models.list()
        return True
    except Exception as e:
        st.error(f"NieprawidÅ‚owy klucz API OpenAI: {e}")
        return False

def export_to_google_sheets(df, spreadsheet_name="Plan TreÅ›ci SEO"):
    """
    Eksportuje DataFrame do Google Sheets z kolorowym formatowaniem.
    """
    try:
        # Pobierz credentials z Streamlit secrets
        creds_dict = st.secrets.get("gcp_service_account")
        if not creds_dict:
            st.error("Brak konfiguracji Google Cloud w secrets!")
            return None
        
        # Konwersja do sÅ‚ownika jeÅ›li to string JSON
        if isinstance(creds_dict, str):
            creds_dict = json.loads(creds_dict)
        
        # Tworzenie credentials
        SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        
        # Tworzenie usÅ‚ug
        sheets_service = build('sheets', 'v4', credentials=creds)
        drive_service = build('drive', 'v3', credentials=creds)
        
        # Tworzenie nowego arkusza
        spreadsheet = {
            'properties': {'title': spreadsheet_name},
            'sheets': [{'properties': {'title': 'Plan TreÅ›ci'}}]
        }
        
        spreadsheet = sheets_service.spreadsheets().create(body=spreadsheet).execute()
        spreadsheet_id = spreadsheet['spreadsheetId']
        
        # Przygotowanie danych - konwersja wszystkich wartoÅ›ci na stringi
        df_export = df.copy()
        # Konwersja wszystkich wartoÅ›ci na string aby uniknÄ…Ä‡ bÅ‚Ä™dÃ³w typÃ³w
        for col in df_export.columns:
            df_export[col] = df_export[col].astype(str).replace('nan', '').replace('None', '')
        
        headers = [df_export.columns.tolist()]
        values = df_export.values.tolist()
        all_data = headers + values
        
        # Wpisanie danych
        body = {'values': all_data}
        sheets_service.spreadsheets().values().update(
            spreadsheetId=spreadsheet_id,
            range='Plan TreÅ›ci!A1',
            valueInputOption='RAW',
            body=body
        ).execute()
        
        # Formatowanie kolorÃ³w
        requests = []
        
        # Formatowanie nagÅ‚Ã³wkÃ³w
        requests.append({
            'repeatCell': {
                'range': {
                    'sheetId': 0,
                    'startRowIndex': 0,
                    'endRowIndex': 1
                },
                'cell': {
                    'userEnteredFormat': {
                        'backgroundColor': {'red': 0.2, 'green': 0.2, 'blue': 0.2},
                        'textFormat': {'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}, 'bold': True}
                    }
                },
                'fields': 'userEnteredFormat(backgroundColor,textFormat)'
            }
        })
        
        # Kolorowanie wierszy na podstawie statusu
        for idx in range(len(df_export)):
            row = df_export.iloc[idx]
            row_index = idx + 1  # +1 bo nagÅ‚Ã³wek jest w wierszu 0
            status = str(row.get('Status', ''))
            
            color = None
            if status == 'Nowy temat':
                color = {'red': 0.91, 'green': 0.96, 'blue': 0.91}  # Zielony
            elif 'TOP1' in status:
                color = {'red': 1, 'green': 0.95, 'blue': 0.88}  # PomaraÅ„czowy
            elif 'Nie rankuje' in status:
                color = {'red': 1, 'green': 0.92, 'blue': 0.93}  # Czerwony
            
            if color:
                requests.append({
                    'repeatCell': {
                        'range': {
                            'sheetId': 0,
                            'startRowIndex': row_index,
                            'endRowIndex': row_index + 1
                        },
                        'cell': {
                            'userEnteredFormat': {
                                'backgroundColor': color
                            }
                        },
                        'fields': 'userEnteredFormat.backgroundColor'
                    }
                })
        
        # Automatyczne dostosowanie szerokoÅ›ci kolumn
        requests.append({
            'autoResizeDimensions': {
                'dimensions': {
                    'sheetId': 0,
                    'dimension': 'COLUMNS',
                    'startIndex': 0,
                    'endIndex': len(df_export.columns)
                }
            }
        })
        
        # ZamroÅ¼enie pierwszego wiersza
        requests.append({
            'updateSheetProperties': {
                'properties': {
                    'sheetId': 0,
                    'gridProperties': {'frozenRowCount': 1}
                },
                'fields': 'gridProperties.frozenRowCount'
            }
        })
        
        # Zastosowanie formatowania
        if requests:
            body = {'requests': requests}
            sheets_service.spreadsheets().batchUpdate(
                spreadsheetId=spreadsheet_id,
                body=body
            ).execute()
        
        # UdostÄ™pnianie arkusza (opcjonalnie - kaÅ¼dy z linkiem moÅ¼e oglÄ…daÄ‡)
        permission = {
            'type': 'anyone',
            'role': 'reader'
        }
        drive_service.permissions().create(
            fileId=spreadsheet_id,
            body=permission
        ).execute()
        
        spreadsheet_url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}"
        return spreadsheet_url
        
    except Exception as e:
        st.error(f"BÅ‚Ä…d podczas eksportu do Google Sheets: {e}")
        return None

# --- Interfejs UÅ¼ytkownika (UI) ---
st.title("ğŸš€ Planer TreÅ›ci SEO [Wersja HDBSCAN v9 - ULTIMATE]")
st.markdown("âœ¨ Zaawansowana klasteryzacja z HDBSCAN, analiza intencji, scoring priorytetÃ³w i rozszerzony status rankingu.")

col1, col2 = st.columns(2)
with col1:
    st.header("1. Konfiguracja")
    num_to_generate = st.number_input("Liczba nowych artykuÅ‚Ã³w do wygenerowania", min_value=1, value=20)
    similarity_threshold = st.slider("PrÃ³g podobieÅ„stwa dla optymalizacji", min_value=0.7, max_value=1.0, value=0.8, step=0.01)
    
    with st.expander("âš™ï¸ Zaawansowane ustawienia klasteryzacji"):
        min_cluster_size = st.slider(
            "Minimalna wielkoÅ›Ä‡ klastra", 
            min_value=2, 
            max_value=10, 
            value=2,
            help="Im wyÅ¼sza wartoÅ›Ä‡, tym wiÄ™ksze i bardziej ogÃ³lne klastry. WartoÅ›Ä‡ 2-3 zalecana dla SEO."
        )
        min_samples = st.slider(
            "Minimalna liczba prÃ³bek w sÄ…siedztwie", 
            min_value=1, 
            max_value=5, 
            value=1,
            help="Im wyÅ¼sza wartoÅ›Ä‡, tym bardziej konserwatywna klasteryzacja. WartoÅ›Ä‡ 1-2 zalecana."
        )
    
    enable_clustering = st.checkbox("WÅ‚Ä…cz klasteryzacjÄ™ fraz kluczowych (HDBSCAN)", value=True)
    
with col2:
    st.header("2. Wgraj pliki CSV")
    content_gap_file = st.file_uploader("1. Wgraj plik CSV z analizÄ… Content Gap", type="csv")
    my_articles_file = st.file_uploader("2. Wgraj plik CSV z listÄ… swoich artykuÅ‚Ã³w", type="csv")
    ranking_file = st.file_uploader("3. Wgraj plik CSV z aktualnym rankingiem", type="csv")

# --- Logika Aplikacji ---
if st.button("Uruchom AnalizÄ™ HybrydowÄ…", type="primary"):
    openai_api_key = st.secrets.get("OPENAI_API_KEY")
    
    if not openai_api_key:
        st.error("Klucz API OpenAI nie zostaÅ‚ znaleziony w sekretach Streamlit!")
        st.stop()
    
    if not validate_api_key(openai_api_key):
        st.stop()
    
    if not all([content_gap_file, my_articles_file, ranking_file]):
        st.warning("Upewnij siÄ™, Å¼e wgraÅ‚eÅ› wszystkie trzy pliki CSV.")
        st.stop()
    
    with st.spinner("Przeprowadzam analizÄ™..."):
        try:
            df_gap = pd.read_csv(content_gap_file).dropna(subset=['Keyword']).astype({'Keyword': str})
            df_articles = pd.read_csv(my_articles_file)
            df_ranking = pd.read_csv(ranking_file)
            
            df_articles.rename(columns={'Address': 'URL', 'Title 1': 'Title'}, inplace=True)
            df_articles.dropna(subset=['Title', 'URL'], inplace=True)
            df_articles = df_articles[df_articles['Title'].str.strip() != ''].reset_index(drop=True)
            df_articles = df_articles[~df_articles['Title'].str.contains("Bot Verification|Strona|Kategoria", na=False, case=False)].reset_index(drop=True)
            df_ranking.dropna(subset=['SÅ‚owo kluczowe', 'Adres URL'], inplace=True)
            
            # Sprawdzanie czy ranking ma kolumnÄ™ pozycji
            position_col = None
            for col in df_ranking.columns:
                if 'pozycj' in col.lower() or 'position' in col.lower():
                    position_col = col
                    break
            
            st.info(f"Wczytano {len(df_gap)} sÅ‚Ã³w kluczowych, {len(df_articles)} artykuÅ‚Ã³w i {len(df_ranking)} rankingowych sÅ‚Ã³w kluczowych.")
        except Exception as e:
            st.error(f"BÅ‚Ä…d podczas wczytywania plikÃ³w CSV: {e}")
            st.stop()

        # Tworzenie mapy rankingu z pozycjami
        ranking_map = {}
        for _, row in df_ranking.iterrows():
            keyword = str(row['SÅ‚owo kluczowe']).lower()
            url = row['Adres URL']
            position = row[position_col] if position_col and pd.notna(row.get(position_col)) else 0
            ranking_map[keyword] = {'url': url, 'position': position}
        
        results = []
        keywords_for_semantic_check = []
        
        # Pierwsza faza: mapowanie na podstawie rankingu
        for _, row in df_gap.iterrows():
            keyword_lower = str(row['Keyword']).lower()
            if keyword_lower in ranking_map:
                rank_data = ranking_map[keyword_lower]
                position = rank_data['position']
                
                # Rozszerzony status z pozycjÄ…
                if position == 0 or pd.isna(position):
                    status = 'Do optymalizacji (Nie rankuje)'
                elif position <= 3:
                    status = f'Do optymalizacji (Poz. {int(position)} â†’ TOP1)'
                elif position <= 10:
                    status = f'Do optymalizacji (Poz. {int(position)} â†’ TOP3)'
                elif position <= 20:
                    status = f'Do optymalizacji (Poz. {int(position)} â†’ TOP10)'
                else:
                    status = f'Do optymalizacji (Poz. {int(position)} â†’ TOP20)'
                
                results.append({
                    'SÅ‚owo kluczowe': row['Keyword'],
                    'Wolumen': row.get('Volume', 0),
                    'Status': status,
                    'Akcja / Dopasowany URL': rank_data['url'],
                    'PodobieÅ„stwo': 1.00,
                    'Aktualna_pozycja': position
                })
            else:
                keywords_for_semantic_check.append(row.to_dict())
        
        st.info(f"{len(results)} sÅ‚Ã³w zmapowano na podstawie rankingu. PozostaÅ‚o {len(keywords_for_semantic_check)} do analizy semantycznej.")
        
        cluster_stats = None
        
        if keywords_for_semantic_check:
            df_semantic = pd.DataFrame(keywords_for_semantic_check)
            
            # Upewniamy siÄ™, Å¼e kolumna Wolumen istnieje
            if 'Volume' in df_semantic.columns and 'Wolumen' not in df_semantic.columns:
                df_semantic['Wolumen'] = df_semantic['Volume']
            elif 'Wolumen' not in df_semantic.columns:
                df_semantic['Wolumen'] = 0
            
            # Dodajemy kolumnÄ™ SÅ‚owo kluczowe jeÅ›li jest Keyword
            if 'Keyword' in df_semantic.columns and 'SÅ‚owo kluczowe' not in df_semantic.columns:
                df_semantic['SÅ‚owo kluczowe'] = df_semantic['Keyword']
            
            st.info("ğŸ”„ Generowanie embeddingÃ³w dla artykuÅ‚Ã³w...")
            
            # Generowanie embeddingÃ³w
            corpus_embeddings = get_openai_embeddings(
                tuple(df_articles['Title'].tolist()),
                openai_api_key
            )
            
            st.info("ğŸ”„ Generowanie embeddingÃ³w dla sÅ‚Ã³w kluczowych...")
            
            query_embeddings = get_openai_embeddings(
                tuple(df_semantic['Keyword' if 'Keyword' in df_semantic.columns else 'SÅ‚owo kluczowe'].tolist()),
                openai_api_key
            )

            if not corpus_embeddings or not query_embeddings:
                st.error("Nie udaÅ‚o siÄ™ wygenerowaÄ‡ wektorÃ³w. SprawdÅº klucz API i sprÃ³buj ponownie.")
                st.stop()
            
            # Klasteryzacja fraz (jeÅ›li wÅ‚Ä…czona)
            if enable_clustering:
                st.info("ğŸ”„ KlasteryzujÄ™ frazy kluczowe z HDBSCAN...")
                df_semantic = cluster_keywords_hdbscan(
                    df_semantic, 
                    query_embeddings, 
                    min_cluster_size=min_cluster_size,
                    min_samples=min_samples
                )
                
                # Analiza jakoÅ›ci klasteryzacji
                cluster_stats = analyze_cluster_coherence(df_semantic, query_embeddings)
            
            # Analiza semantyczna
            cosine_scores = util.cos_sim(torch.tensor(query_embeddings), torch.tensor(corpus_embeddings))
            
            semantic_records = df_semantic.to_dict('records')
            for row_dict, scores in zip(semantic_records, cosine_scores):
                top_result = torch.topk(scores, k=1)
                score, corpus_idx = top_result.values[0].item(), top_result.indices[0].item()
                closest_article_url = df_articles.iloc[corpus_idx]['URL']
                closest_article_title = df_articles.iloc[corpus_idx]['Title']
                
                # Pobieramy sÅ‚owo kluczowe z odpowiedniej kolumny
                keyword = row_dict.get('Keyword', row_dict.get('SÅ‚owo kluczowe', ''))
                volume = row_dict.get('Volume', row_dict.get('Wolumen', 0))

                if score > similarity_threshold:
                    status = f'Do optymalizacji (Podobne do: {closest_article_title[:50]}...)'
                    url = closest_article_url
                else:
                    status = 'Nowy temat'
                    url = 'StwÃ³rz nowy artykuÅ‚'
                
                # Wykrywanie intencji
                intent = detect_search_intent(keyword)
                
                result = {
                    'SÅ‚owo kluczowe': keyword,
                    'Wolumen': volume,
                    'Status': status,
                    'Akcja / Dopasowany URL': url,
                    'NajbliÅ¼szy_artykuÅ‚': closest_article_url,
                    'NajbliÅ¼szy_tytuÅ‚': closest_article_title,
                    'PodobieÅ„stwo': round(score, 2),
                    'Intencja': intent,
                    'Aktualna_pozycja': 0
                }
                
                # Dodanie informacji o klastrze
                if enable_clustering:
                    result['Klaster_ID'] = row_dict.get('Klaster_ID', 0)
                    result['HEAD_Keyword'] = row_dict.get('HEAD_Keyword', keyword)
                    result['Typ_w_klastrze'] = row_dict.get('Typ_w_klastrze', 'HEAD')
                    result['Liczba_fraz_w_klastrze'] = row_dict.get('Liczba_fraz_w_klastrze', 1)
                    result['Jest_Outlier'] = row_dict.get('Jest_Outlier', False)
                    result['Cluster_Probability'] = round(row_dict.get('Cluster_Probability', 1.0), 3)
                    result['Cluster_Quality'] = round(row_dict.get('Cluster_Quality', 1.0), 3)
                
                results.append(result)
        
        df_results = pd.DataFrame(results)
        
        # Obliczanie scoring priorytetu
        df_results['Priorytet_Score'] = df_results.apply(calculate_priority_score, axis=1)
        
        # Generowanie tytuÅ‚Ã³w tylko dla HEAD keywords w klastrach
        if enable_clustering:
            df_new_topics = df_results[
                (df_results['Status'] == 'Nowy temat') & 
                (df_results['Typ_w_klastrze'] == 'HEAD')
            ].copy()
        else:
            df_new_topics = df_results[df_results['Status'] == 'Nowy temat'].copy()
        
        if not df_new_topics.empty:
            df_to_process = df_new_topics.sort_values(by='Priorytet_Score', ascending=False).head(num_to_generate)
            st.info(f"GenerujÄ™ propozycje tytuÅ‚Ã³w dla {len(df_to_process)} najwaÅ¼niejszych nowych tematÃ³w...")
            
            df_gap_indexed = df_gap.set_index('Keyword')
            df_to_process['Competitor URL'] = df_to_process['SÅ‚owo kluczowe'].map(
                df_gap_indexed.apply(find_first_competitor_url, axis=1)
            )

            progress_bar = st.progress(0, text="Generowanie tytuÅ‚Ã³w (GPT-4o)...")
            
            generated_titles_data = []
            for i, (idx, row) in enumerate(df_to_process.iterrows()):
                # Zbieranie powiÄ…zanych fraz z klastra
                related_keywords = ""
                if enable_clustering and row.get('Liczba_fraz_w_klastrze', 1) > 1:
                    related = df_results[
                        (df_results['Klaster_ID'] == row['Klaster_ID']) & 
                        (df_results['Typ_w_klastrze'] == 'RELATED')
                    ]['SÅ‚owo kluczowe'].tolist()
                    related_keywords = ", ".join(related[:5])  # Max 5 powiÄ…zanych fraz
                
                titles = generate_titles(
                    openai_api_key,
                    row['SÅ‚owo kluczowe'],
                    row['Wolumen'],
                    row.get('Competitor URL', 'Brak'),
                    related_keywords
                )
                generated_titles_data.append({
                    'SÅ‚owo kluczowe': row['SÅ‚owo kluczowe'],
                    'Propozycja_tematu_1': titles[0],
                    'Propozycja_tematu_2': titles[1],
                    'Propozycja_tematu_3': titles[2]
                })
                progress_bar.progress((i + 1) / len(df_to_process), text=f"Generowanie tytuÅ‚Ã³w ({i+1}/{len(df_to_process)})")

            if generated_titles_data:
                df_titles = pd.DataFrame(generated_titles_data)
                df_results = pd.merge(df_results, df_titles, on='SÅ‚owo kluczowe', how='left')

        df_results.fillna('', inplace=True)  # Zmienione z '-' na ''
        st.success("âœ… Analiza zakoÅ„czona!")
        
        # Statystyki - rozszerzone dla HDBSCAN
        if enable_clustering and cluster_stats:
            st.subheader("ğŸ“ˆ Statystyki Klasteryzacji HDBSCAN")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric("Wykryte klastry", cluster_stats['total_clusters'])
            with col2:
                st.metric("Frazy outlier", cluster_stats['outliers'])
            with col3:
                st.metric("Åšr. wielkoÅ›Ä‡ klastra", f"{cluster_stats['avg_cluster_size']:.1f}")
            with col4:
                st.metric("Maks. wielkoÅ›Ä‡ klastra", cluster_stats['max_cluster_size'])
            with col5:
                st.metric("Nowe tematy", len(df_results[df_results['Status'] == 'Nowy temat']))
            
            # SzczegÃ³Å‚y najwaÅ¼niejszych klastrÃ³w
            with st.expander("ğŸ” SzczegÃ³Å‚y najwaÅ¼niejszych klastrÃ³w (TOP 10)"):
                clusters_df = pd.DataFrame(cluster_stats['clusters_details'])
                if not clusters_df.empty:
                    clusters_df = clusters_df.sort_values('total_volume', ascending=False).head(10)
                    for _, cluster in clusters_df.iterrows():
                        st.markdown(f"**Klaster {cluster['cluster_id']}** (WielkoÅ›Ä‡: {cluster['size']}, SpÃ³jnoÅ›Ä‡: {cluster['avg_similarity']:.2f}, Wolumen: {cluster['total_volume']})")
                        st.markdown(f"PrzykÅ‚adowe frazy: {', '.join(cluster['keywords'])}")
                        st.markdown("---")
        else:
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Nowe tematy", len(df_results[df_results['Status'] == 'Nowy temat']))
            with col2:
                st.metric("Do optymalizacji", len(df_results[df_results['Status'].str.contains('optymalizacji', na=False)]))
            with col3:
                if enable_clustering:
                    st.metric("Liczba klastrÃ³w", df_results.get('Klaster_ID', pd.Series()).nunique())
            with col4:
                st.metric("Åšredni priorytet", f"{df_results['Priorytet_Score'].mean():.1f}")
        
       # --- POCZÄ„TEK NOWEGO BLOKU WYÅšWIETLANIA I EKSPORTU ---

# Ta sekcja uruchomi siÄ™ tylko wtedy, gdy analiza zostaÅ‚a przeprowadzona i wyniki sÄ… w pamiÄ™ci
if st.session_state.analysis_results is not None:
    
    st.header("ğŸ“Š Wyniki Analizy i Plan TreÅ›ci")
    
    # Pobieramy wyniki z session_state
    df_results_sorted = st.session_state.analysis_results
    
    # Definicja kolejnoÅ›ci kolumn
    cols_order = [
        'SÅ‚owo kluczowe', 'Wolumen', 'Priorytet_Score', 'Status', 
        'Akcja / Dopasowany URL', 'NajbliÅ¼szy_artykuÅ‚', 'PodobieÅ„stwo',
        'Intencja', 'Aktualna_pozycja'
    ]
    
    if enable_clustering:
        cols_order.extend([
            'Klaster_ID', 'HEAD_Keyword', 'Typ_w_klastrze', 'Liczba_fraz_w_klastrze',
            'Jest_Outlier', 'Cluster_Probability', 'Cluster_Quality'
        ])
    
    cols_order.extend(['Propozycja_tematu_1', 'Propozycja_tematu_2', 'Propozycja_tematu_3'])
    
    existing_cols = [col for col in cols_order if col in df_results_sorted.columns]
    
    # Dodanie kolorowania dla lepszej wizualizacji
    def highlight_rows(row):
        if row['Status'] == 'Nowy temat':
            return ['background-color: #e8f5e9'] * len(row)
        elif 'TOP1' in str(row['Status']):
            return ['background-color: #fff3e0'] * len(row)
        elif 'Nie rankuje' in str(row['Status']):
            return ['background-color: #ffebee'] * len(row)
        return [''] * len(row)
    
    st.dataframe(
        df_results_sorted[existing_cols].style.apply(highlight_rows, axis=1),
        use_container_width=True, # UÅ¼yj use_container_width zamiast width
        height=600
    )
    
    # Legenda kolorÃ³w
    st.markdown("""
    **Legenda kolorÃ³w:**
    - ğŸŸ¢ Zielony: Nowy temat
    - ğŸŸ  PomaraÅ„czowy: Blisko TOP1 (optymalizacja)
    - ğŸ”´ Czerwony: Nie rankuje
    """)
    
    # Export do CSV i Google Sheets
    csv_buffer = io.StringIO()
    df_results_sorted[existing_cols].to_csv(csv_buffer, index=False, encoding='utf-8')
    csv_bytes = csv_buffer.getvalue().encode('utf-8-sig')

    col_download1, col_download2 = st.columns(2)
    
    with col_download1:
        st.download_button(
            "ğŸ“¥ Pobierz jako CSV (bez kolorÃ³w)", 
            csv_bytes, 
            "plan_tresci_hdbscan_ultimate.csv", 
            "text/csv"
        )
    
    with col_download2:
        if st.button("ğŸ“Š Eksportuj do Google Sheets (z kolorami)", type="primary"):
            with st.spinner("TworzÄ™ Google Sheets z formatowaniem..."):
                sheets_url = export_to_google_sheets(
                    df_results_sorted[existing_cols],
                    f"Plan TreÅ›ci SEO - {time.strftime('%Y-%m-%d %H:%M')}"
                )
                if sheets_url:
                    st.success("âœ… Arkusz utworzony!")
                    st.markdown(f"ğŸ”— [OtwÃ³rz w Google Sheets]({sheets_url})")
                    st.code(sheets_url, language=None)
    
    # Dodatkowe eksporty
    col_export1, col_export2 = st.columns(2)
    
    with col_export1:
        if enable_clustering:
            df_head_only = df_results_sorted[df_results_sorted['Typ_w_klastrze'] == 'HEAD']
            csv_head_buffer = io.StringIO()
            df_head_only[existing_cols].to_csv(csv_head_buffer, index=False, encoding='utf-8')
            csv_head_bytes = csv_head_buffer.getvalue().encode('utf-8-sig')
            
            st.download_button(
                "ğŸ“¥ Pobierz tylko HEAD keywords", 
                csv_head_bytes, 
                "plan_tresci_head_only.csv", 
                "text/csv"
            )
    
    with col_export2:
        df_priority = df_results_sorted[df_results_sorted['Status'] == 'Nowy temat'].head(50)
        csv_priority_buffer = io.StringIO()
        df_priority[existing_cols].to_csv(csv_priority_buffer, index=False, encoding='utf-8')
        csv_priority_bytes = csv_priority_buffer.getvalue().encode('utf-8-sig')
        
        st.download_button(
            "ğŸ“¥ Pobierz TOP 50 priorytetÃ³w", 
            csv_priority_bytes, 
            "plan_tresci_top50.csv", 
            "text/csv"
        )
    
    # Dodatkowa analiza
    st.header("ğŸ“Š Dodatkowa Analiza")
    
    tab1, tab2, tab3 = st.tabs(["RozkÅ‚ad intencji", "Analiza wolumenu", "JakoÅ›Ä‡ klastrÃ³w"])
    
    with tab1:
        if 'Intencja' in df_results_sorted.columns:
            intent_counts = df_results_sorted['Intencja'].value_counts()
            st.bar_chart(intent_counts)
            st.markdown("**Interpretacja:** DominujÄ…ca intencja wyszukiwania w analizowanych frazach.")
    
    with tab2:
        volume_by_status = df_results_sorted.groupby('Status')['Wolumen'].sum().sort_values(ascending=False)
        st.bar_chart(volume_by_status)
        st.markdown("**Interpretacja:** ÅÄ…czny potencjaÅ‚ ruchu dla kaÅ¼dej kategorii statusu.")
    
    with tab3:
        if enable_clustering and 'Cluster_Quality' in df_results_sorted.columns:
            quality_series = df_results_sorted[df_results_sorted['Typ_w_klastrze'] == 'HEAD']['Cluster_Quality']
            quality_dist = pd.to_numeric(quality_series, errors='coerce').dropna()
            
            if len(quality_dist) > 0:
                st.line_chart(quality_dist.sort_values(ascending=False))
                st.markdown(f"**Åšrednia jakoÅ›Ä‡ klastrÃ³w:** {quality_dist.mean():.3f}")
                st.markdown("**Interpretacja:** Im wyÅ¼sza wartoÅ›Ä‡, tym bardziej spÃ³jne semantycznie sÄ… frazy w klastrze.")
                
                weak_clusters = df_results_sorted[
                    (df_results_sorted['Typ_w_klastrze'] == 'HEAD') & 
                    (pd.to_numeric(df_results_sorted['Cluster_Quality'], errors='coerce') < 0.5)
                ]['SÅ‚owo kluczowe'].tolist()
                
                if weak_clusters:
                    st.warning(f"âš ï¸ Znaleziono {len(weak_clusters)} klastrÃ³w o niskiej spÃ³jnoÅ›ci. RozwaÅ¼ ich weryfikacjÄ™ rÄ™cznÄ….")
                    with st.expander("Zobacz listÄ™"):
                        for kw in weak_clusters[:10]:
                            st.markdown(f"- {kw}")
            else:
                st.info("Brak danych o jakoÅ›ci klastrÃ³w do wyÅ›wietlenia.")
# --- KONIEC NOWEGO BLOKU ---
